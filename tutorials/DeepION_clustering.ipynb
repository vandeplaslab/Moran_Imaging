{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb0beffd-d06c-48b8-bf4c-cecadbdb73d2",
   "metadata": {},
   "source": [
    "### Colocalization-based ion image clusternig with DeepION\n",
    "\n",
    "Publication: \"DeepION: A Deep Learning-Based Low-Dimensional Representation Model of Ion Images for Mass Spectrometry Imaging\" by Lei Guo. Analytical Chemistry. 2024. https://pubs.acs.org/doi/full/10.1021/acs.analchem.3c05002\n",
    "\n",
    "Code by Lei Guo: https://github.com/gankLei-X/DeepION/tree/main\n",
    "\n",
    "### Notes\n",
    "\n",
    "This code was implemented using PyTorch and is best performed on a GPU-enabled system, although, it can also run on a CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d38445b1-f1d4-4395-870d-37fd416f78eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lgmigas/Documents/GitHub/Moran_Imaging/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import pickle\n",
    "import random\n",
    "from pathlib import Path\n",
    "from itertools import chain\n",
    "\n",
    "\n",
    "import torch\n",
    "import umap\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import adjusted_rand_score, adjusted_mutual_info_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.cluster import HDBSCAN\n",
    "\n",
    "# Import necessary Moran Imaging modules\n",
    "from moran_imaging.ari_balance import balanced_adjusted_rand_index\n",
    "from moran_imaging.vmeasure_balance import balanced_homogeneity_completeness_v_measure\n",
    "\n",
    "# Import necessary DeepION module\n",
    "from moran_imaging._torch import get_backend\n",
    "from moran_imaging.deep_ion_clustering import DeepION_training, DeepION_predicting\n",
    "\n",
    "# Fix random seed\n",
    "random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "008d2b73-f73e-4d27-b7b6-8916e3f1eed3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "0\n",
      "No GPU found, using CPU instead.\n"
     ]
    }
   ],
   "source": [
    "# Verify GPU availability\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.device_count())\n",
    "if torch.cuda.is_available():\n",
    "    print(torch.cuda.get_device_name(0))  \n",
    "    print(\"Torch CUDA Version:\", torch.version.cuda)\n",
    "    print(\"Torch Backend CUDA Available:\", torch.backends.cudnn.enabled)\n",
    "\n",
    "    # Set CUDA device \n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
    "else:\n",
    "    print(\"No GPU found, using CPU instead.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d7e1b32c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using backend: mps\n"
     ]
    }
   ],
   "source": [
    "backend = get_backend()\n",
    "print(f\"Using backend: {backend}\")\n",
    "torch.set_default_device(backend)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f08ddee1-0997-4cbd-8fbe-4e2a16d13b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_ion_image(data, image_shape, background_mask, fill_zeros=True, show_image=True, colormap='coolwarm'):\n",
    "    # Fill background pixels with zeros (default) or NaN\n",
    "    pixel_grid = np.zeros((image_shape[0]*image_shape[1], ))\n",
    "    pixel_grid[np.invert(background_mask)] = data\n",
    "    \n",
    "    if not fill_zeros:\n",
    "        pixel_grid[background_mask] = np.nan\n",
    "    \n",
    "    # Reshape data\n",
    "    ion_image = np.reshape(pixel_grid, image_shape)\n",
    "    \n",
    "    # Plot ion image \n",
    "    if show_image: \n",
    "        plt.figure(dpi=100)\n",
    "        plt.imshow(ion_image, cmap=colormap)\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "    return ion_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a688eedf-b816-445a-8ade-1b9c039e3d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to train DeepION model\n",
    "def train_deepion(input_matrix, input_shape, mode=\"COL\", results_dir=None):\n",
    "    \"\"\"\n",
    "    Runs DeepION training with the given MSI dataset.\n",
    "    \n",
    "    Parameters:\n",
    "        input_matrix (str): Path to the MSI data matrix file.\n",
    "        input_shape (tuple): Shape of the MSI dataset (height, width).\n",
    "        mode (str): Mode of operation, either 'COL' (colocalization) or 'ISO' (isotope discovery).\n",
    "        results_dir (str): Directory to save the trained model and results.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    print(\"Starting DeepION training...\")\n",
    "    DeepION_training(input_matrix, input_shape, mode, model_dir=results_dir)\n",
    "    print(\"Training completed!\")\n",
    "\n",
    "# Function to run DeepION feature extraction\n",
    "def predict_deepion(input_matrix, input_shape, mode=\"COL\", results_dir=None):\n",
    "    \"\"\"\n",
    "    Runs DeepION feature extraction on the given MSI dataset.\n",
    "    \n",
    "    Parameters:\n",
    "        input_matrix (str): Path to the MSI data matrix file.\n",
    "        input_shape (tuple): Shape of the MSI dataset (height, width).\n",
    "        mode (str): Mode of operation, either 'COL' (colocalization) or 'ISO' (isotope discovery).\n",
    "        results_dir (str): Directory to save the trained model and results.\n",
    "    \n",
    "    Returns:\n",
    "        np.ndarray: Extracted feature matrix.\n",
    "    \"\"\"\n",
    "    print(\"Extracting features using DeepION...\")\n",
    "    features = DeepION_predicting(input_matrix, input_shape, mode, model_dir=results_dir)\n",
    "    print(\"Feature extraction completed!\")\n",
    "    return features\n",
    "\n",
    "# Function to apply dimensionality reduction\n",
    "def reduce_dimensionality(features):\n",
    "    \"\"\"\n",
    "    Applies dimensionality reduction to extracted features.\n",
    "    \n",
    "    Parameters:\n",
    "        features (np.ndarray): Feature matrix extracted from DeepION.\n",
    "    \n",
    "    Returns:\n",
    "        np.ndarray: Low-dimensional feature representation.\n",
    "    \"\"\"\n",
    "    print(\"Applying dimensionality reduction...\")\n",
    "\n",
    "    features_umap = umap.UMAP(n_components=20, metric='cosine', random_state=0).fit_transform(features)\n",
    "    features_umap = MinMaxScaler().fit_transform(features_umap)\n",
    "    \n",
    "    print(\"Dimensionality reduction completed!\")\n",
    "    return features_umap\n",
    "\n",
    "# # Function to run co-localized ion searching\n",
    "# def run_colocalized_search(ld_features, peak_list, num=5, output_file=\"output/\"):\n",
    "#     \"\"\"\n",
    "#     Runs co-localized ion searching using DeepION features.\n",
    "    \n",
    "#     Parameters:\n",
    "#         ld_features (np.ndarray): Low-dimensional feature representation.\n",
    "#         peak_list (str): Path to MSI peak list file.\n",
    "#         num (int): Number of co-localized ions to search for each ion.\n",
    "#         output_file (str): Path to save results.\n",
    "#     \"\"\"\n",
    "#     print(f\"Running co-localized ion searching (Top {num} ions)...\")\n",
    "#     Co_localizedIONSearching(ld_features, peak_list, num, output_file)\n",
    "#     print(f\"Results saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a9fe2bc-c1d3-4e13-886a-b6aef03e8307",
   "metadata": {},
   "source": [
    "#### Transform zebra-fish dataset into a DeepION-compatible format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b2ab545f-7716-4c02-aea2-883b8ed23695",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "# The data is expected to be located in the 'Data' directory relative to the current working directory.\n",
    "data_dir = Path(\".\").resolve() / \"Data\"\n",
    "assert data_dir.exists(), f\"The data directory {data_dir} does not exist.\"\n",
    "\n",
    "# As an example, we are using the Zebra fish dataset with 8 clusters\n",
    "pickle_file = data_dir / \"Zebra_fish_8_clusters_dataset.pickle\"\n",
    "assert pickle_file.exists(), f\"The pickle file {pickle_file} does not exist.\"\n",
    "\n",
    "with open(pickle_file, 'rb') as file:\n",
    "    clustering_data_metadata = pickle.load(file) \n",
    "\n",
    "# create results directory\n",
    "results_dir = data_dir.parent / \"Results\" / \"DeepION_Clustering\"\n",
    "results_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "image_shape = clustering_data_metadata['utils']['image_shape']\n",
    "background_mask = clustering_data_metadata['utils']['background_mask']\n",
    "data_cluster1 = clustering_data_metadata['cluster_1']['data'] \n",
    "data_cluster2 = clustering_data_metadata['cluster_2']['data']\n",
    "data_cluster3 = clustering_data_metadata['cluster_3']['data']\n",
    "data_cluster4 = clustering_data_metadata['cluster_4']['data']\n",
    "data_cluster5 = clustering_data_metadata['cluster_5']['data']\n",
    "data_cluster6 = clustering_data_metadata['cluster_6']['data']\n",
    "data_cluster7 = clustering_data_metadata['cluster_7']['data']\n",
    "data_cluster8 = clustering_data_metadata['cluster_8']['data']\n",
    "\n",
    "# Reference cluster labels\n",
    "ref_labels = (\n",
    "    [1]*data_cluster1.shape[1] + \n",
    "    [2]*data_cluster2.shape[1] + \n",
    "    [3]*data_cluster3.shape[1] + \n",
    "    [4]*data_cluster4.shape[1] + \n",
    "    [5]*data_cluster5.shape[1] +\n",
    "    [6]*data_cluster6.shape[1] + \n",
    "    [7]*data_cluster7.shape[1] + \n",
    "    [8]*data_cluster8.shape[1]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6439e3c0-b428-4b6d-a64b-a9af3267e8a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add off-tissue pixels to each ion image\n",
    "deepion_matrix_file = results_dir / \"zebra_fish_for_DeepION_matrix.txt\"\n",
    "if not deepion_matrix_file.exists():\n",
    "    dataset = np.hstack((data_cluster1, data_cluster2, data_cluster3, data_cluster4, data_cluster5, data_cluster6, data_cluster7, data_cluster8))\n",
    "    total_num_pixels = np.prod(image_shape)\n",
    "    num_mz_bins = dataset.shape[1]\n",
    "\n",
    "    dataset_final = np.zeros((total_num_pixels, num_mz_bins))\n",
    "    for mz_index in range(num_mz_bins):\n",
    "        ion_image_flat = dataset[:, mz_index]\n",
    "        ion_image = make_ion_image(ion_image_flat, image_shape, background_mask, fill_zeros=True, show_image=False)\n",
    "        dataset_final[:, mz_index] = ion_image.flatten()\n",
    "    # Save to text file for DeepION\n",
    "    np.savetxt(deepion_matrix_file, dataset_final)\n",
    "    del dataset, dataset_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d97e6676-f508-4d9d-a230-6271288edebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "deepion_peaks_file = results_dir / \"zebra_fish_for_DeepION_peaks.txt\"\n",
    "if not deepion_peaks_file.exists():\n",
    "    # Mass-to-charge ratios of the molecular species in each cluster\n",
    "    mz_per_cluster = {}\n",
    "    mz_per_cluster[1] = clustering_data_metadata['cluster_1']['mz'].tolist()\n",
    "    mz_per_cluster[2] = clustering_data_metadata['cluster_2']['mz'].tolist()\n",
    "    mz_per_cluster[3] = clustering_data_metadata['cluster_3']['mz'].tolist()\n",
    "    mz_per_cluster[4] = clustering_data_metadata['cluster_4']['mz'].tolist()\n",
    "    mz_per_cluster[5] = clustering_data_metadata['cluster_5']['mz'].tolist()\n",
    "    mz_per_cluster[6] = clustering_data_metadata['cluster_6']['mz'].tolist()\n",
    "    mz_per_cluster[7] = clustering_data_metadata['cluster_7']['mz'].tolist()\n",
    "    mz_per_cluster[8] = clustering_data_metadata['cluster_8']['mz'].tolist()\n",
    "\n",
    "    mz_list = []\n",
    "    for cluster_label in range(1, 8): \n",
    "        mz_list.append(mz_per_cluster[cluster_label])\n",
    "\n",
    "    mz_list = np.array(list(chain(*mz_list)))\n",
    "\n",
    "    # Save as a CSV file for DeepION\n",
    "    np.savetxt(deepion_peaks_file, mz_list, delimiter=\",\", fmt=\"%d\", comments='')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e25e40-44d7-4cfa-8fc1-e80eb49920f2",
   "metadata": {},
   "source": [
    "#### DeepION clustering workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7f5ef603-ed8f-49e9-8cab-338fb6490487",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting DeepION training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training DeepION Model: 100%|██████████| 200/200 [1:04:17<00:00, 19.29s/epoch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed!\n",
      "DeepION execution time: 3889.937629 seconds\n",
      "Extracting features using DeepION...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Features: 100%|██████████| 1/1 [00:05<00:00,  5.91s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature extraction completed!\n",
      "DeepION prediction execution time: 36.049775 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "input_shape = image_shape # (height, width)\n",
    "mode = \"COL\"  # \"COL\" for colocalization\n",
    "ion_mode = \"positive\"\n",
    "num_co_localized = 10\n",
    "\n",
    "train_deepion(deepion_matrix_file, input_shape, mode, results_dir=results_dir)\n",
    "train_time = time.time() - start_time\n",
    "print(f\"DeepION execution time: {train_time:.6f} seconds\")\n",
    "\n",
    "features = predict_deepion(deepion_matrix_file, input_shape, mode, results_dir=results_dir)\n",
    "predict_time = time.time() - start_time - train_time\n",
    "print(f\"DeepION prediction execution time: {predict_time:.6f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0056a1ec-0829-4b17-9e48-fdb1f151a3bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying dimensionality reduction...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lgmigas/Documents/GitHub/Moran_Imaging/venv/lib/python3.10/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensionality reduction completed!\n",
      "UMAP execution time: 4.661836 seconds\n"
     ]
    }
   ],
   "source": [
    "# Dimensionality reduction with UMAP\n",
    "features_umap = reduce_dimensionality(features)\n",
    "umap_time = time.time() - start_time - train_time - predict_time\n",
    "print(f\"UMAP execution time: {umap_time:.6f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "28515793-a301-4650-9988-0d4a2999e256",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HDBSCAN execution time: 0.026255 seconds\n"
     ]
    }
   ],
   "source": [
    "# HDBSCAN clustering\n",
    "HDBSCAN_model = HDBSCAN(min_cluster_size=5, max_cluster_size=50, metric=\"euclidean\", n_jobs=-1)\n",
    "HDBSCAN_model.fit(features_umap)\n",
    "DeepION_labels = HDBSCAN_model.labels_\n",
    "hdbscan_time = time.time() - start_time - train_time - predict_time - umap_time\n",
    "print(f\"HDBSCAN execution time: {hdbscan_time:.6f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c1917d00-7dc5-4d4e-8bf6-bde0b32edcfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DeepION execution time: 3930.679492 seconds\n"
     ]
    }
   ],
   "source": [
    "end_time = time.time()\n",
    "compute_time = end_time - start_time\n",
    "print(f\"DeepION execution time: {compute_time:.6f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7ba65b7f-b02f-49ff-acc5-841d9eda2444",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusted Rand index: 0.5635\n",
      "Adjusted mutual information: 0.7711\n",
      "Balanced adjusted Rand index: 0.7508\n",
      "Balanced V-measure: 0.8704\n"
     ]
    }
   ],
   "source": [
    "# Clustering performance \n",
    "ARI = np.round(adjusted_rand_score(ref_labels, DeepION_labels), 4)\n",
    "print('Adjusted Rand index:', ARI)\n",
    "AMI = np.round(adjusted_mutual_info_score(ref_labels, DeepION_labels, average_method='arithmetic'), 4)\n",
    "print('Adjusted mutual information:', AMI)\n",
    "BARI = np.round(balanced_adjusted_rand_index(np.array(ref_labels), np.array(DeepION_labels)), 4)\n",
    "print('Balanced adjusted Rand index:', BARI)\n",
    "balanced_homogeneity, balanced_completeness, balanced_v_measure = np.round(balanced_homogeneity_completeness_v_measure(np.array(ref_labels), np.array(DeepION_labels)), 4)\n",
    "print('Balanced V-measure:', balanced_v_measure)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
